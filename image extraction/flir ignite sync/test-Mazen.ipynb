{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0817d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder metadata...\n",
      "Detected oldest image from: 2025-12-02 00:00:00\n",
      "Connecting to DB to check records since 2025-12-02 00:00:00...\n",
      "Database holds 3 potential duplicates in this time range.\n",
      "\n",
      "No new data found. (Skipped 3 duplicates).\n"
     ]
    }
   ],
   "source": [
    "################################### AUTO THERMAL PIPELINE (WATCHDOG + ETL) #################### \n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import base64\n",
    "import io\n",
    "import flyr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FOLDER = \"flir e5 photodump\"\n",
    "EXIFTOOL_PATH = \"exiftool-12.35.exe\" \n",
    "\n",
    "# --- DATABASE CREDENTIALS ---\n",
    "DB_SERVER = \"PSQLAPPEG297-01\"\n",
    "DB_NAME = \"Flir\"\n",
    "DB_USER = \"Flir\"\n",
    "DB_PASS = \"Prom@2025\"\n",
    "DB_TABLE = \"ThermalReadings\"\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: DATABASE & HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def get_db_engine():\n",
    "    \"\"\"Creates the connection to MSSQL.\"\"\"\n",
    "    encoded_pass = quote_plus(DB_PASS)\n",
    "    db_url = f\"mssql+pyodbc://{DB_USER}:{encoded_pass}@{DB_SERVER}/{DB_NAME}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    return create_engine(db_url)\n",
    "\n",
    "def get_existing_signatures(engine, start_date_str):\n",
    "    \"\"\"\n",
    "    SMART FILTER: Queries DB for records newer than the oldest local photo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT Asset_Name, Timestamp FROM {DB_TABLE} WHERE Timestamp >= '{start_date_str}'\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        \n",
    "        if not df.empty:\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='mixed')\n",
    "            signatures = set(zip(\n",
    "                df['Asset_Name'], \n",
    "                df['Timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            ))\n",
    "            return signatures\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Database check failed (Table might be empty): {e}\")\n",
    "        return set()\n",
    "\n",
    "def get_metadata(folder):\n",
    "    cmd = [\n",
    "        EXIFTOOL_PATH, '-j', '-n', '-r', \n",
    "        '-DateTimeOriginal', '-CameraSerialNumber', '-ImageDescription', \n",
    "        '-Emissivity', '-ObjectDistance', '-ext', 'jpg', folder\n",
    "    ]\n",
    "    try:\n",
    "        flags = subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, creationflags=flags)\n",
    "        return json.loads(result.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Metadata scan failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_image(filepath, metadata_entry):\n",
    "    filename = os.path.basename(filepath)\n",
    "    asset_name = metadata_entry.get(\"ImageDescription\")\n",
    "    asset_str = str(asset_name).strip() if asset_name else \"\"\n",
    "    \n",
    "    if not asset_str: return None\n",
    "\n",
    "    try:\n",
    "        # Metadata Extraction\n",
    "        serial_int = int(metadata_entry[\"CameraSerialNumber\"])\n",
    "        ts_str = str(metadata_entry[\"DateTimeOriginal\"]).replace(\":\", \"-\", 2)\n",
    "        \n",
    "        # Thermal Extraction\n",
    "        thermogram = flyr.unpack(filepath)\n",
    "        celsius = thermogram.celsius\n",
    "        \n",
    "        # Stats Calculation\n",
    "        h, w = celsius.shape\n",
    "        cy, cx = h // 2, w // 2\n",
    "        center_val = celsius[cy-1:cy+2, cx-1:cx+2].mean()\n",
    "        \n",
    "        row = {\n",
    "            \"Timestamp\": ts_str, \n",
    "            \"Filename\": filename,\n",
    "            \"Camera_Serial\": serial_int,\n",
    "            \"Asset_Name\": asset_str,     \n",
    "            \"Max_Temp_C\": round(celsius.max(), 1),\n",
    "            \"Min_Temp_C\": round(celsius.min(), 1),\n",
    "            \"Avg_Temp_C\": round(celsius.mean(), 1),\n",
    "            \"Center_Temp_C\": round(center_val, 1),\n",
    "            \"Delta_Temp_C\": round(celsius.max() - celsius.min(), 1),\n",
    "            \"Emissivity\": float(metadata_entry.get(\"Emissivity\", 0.95)),\n",
    "            \"Distance\": round(float(metadata_entry.get(\"ObjectDistance\", 1.0)), 1),\n",
    "            \"Image_Base64\": \"\"\n",
    "        }\n",
    "\n",
    "        # Image Generation (JPEG)\n",
    "        buffer = io.BytesIO()\n",
    "        plt.imsave(buffer, celsius, cmap='inferno', format='jpeg')\n",
    "        buffer.seek(0)\n",
    "        raw_b64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        row[\"Image_Base64\"] = f\"data:image/jpeg;base64,{raw_b64}\"\n",
    "        buffer.close()\n",
    "        \n",
    "        return row\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        return None \n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: THE PIPELINE LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def run_pipeline():\n",
    "    print(\"\\nüîÑ Starting Pipeline Run...\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_FOLDER):\n",
    "        print(f\"‚ùå Error: Folder {INPUT_FOLDER} not found\")\n",
    "        return\n",
    "\n",
    "    # 1. Scan Local Metadata\n",
    "    meta_list = get_metadata(INPUT_FOLDER)\n",
    "    if not meta_list:\n",
    "        print(\"   No images found to process.\")\n",
    "        return\n",
    "\n",
    "    meta_dict = {}\n",
    "    timestamps = []\n",
    "    for m in meta_list:\n",
    "        if 'SourceFile' in m:\n",
    "            fname = os.path.basename(m['SourceFile'])\n",
    "            meta_dict[fname] = m\n",
    "            if 'DateTimeOriginal' in m:\n",
    "                timestamps.append(str(m['DateTimeOriginal']).replace(\":\", \"-\", 2))\n",
    "    \n",
    "    if not timestamps:\n",
    "        print(\"   No valid timestamps found.\")\n",
    "        return\n",
    "\n",
    "    # 2. Determine Time Window & Query DB\n",
    "    oldest_photo_time = min(timestamps)\n",
    "    query_start_date = oldest_photo_time[:10] + \" 00:00:00\"\n",
    "    \n",
    "    try:\n",
    "        engine = get_db_engine()\n",
    "        existing_signatures = get_existing_signatures(engine, query_start_date)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DB Connection Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Filter Duplicates\n",
    "    files = [f for f in os.listdir(INPUT_FOLDER) if f.lower().endswith(\".jpg\")]\n",
    "    files_to_process = []\n",
    "    \n",
    "    for f in files:\n",
    "        m_data = meta_dict.get(f, {})\n",
    "        asset = str(m_data.get(\"ImageDescription\", \"\")).strip()\n",
    "        ts = str(m_data.get(\"DateTimeOriginal\", \"\")).replace(\":\", \"-\", 2)[:19]\n",
    "        \n",
    "        if asset and ts and (asset, ts) not in existing_signatures:\n",
    "            files_to_process.append(f)\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(\"‚úÖ No new data. All images are already in Database.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üöÄ Processing {len(files_to_process)} NEW images...\")\n",
    "\n",
    "    # 4. Process & Upload\n",
    "    new_rows = []\n",
    "    for f in files_to_process:\n",
    "        full_path = os.path.join(INPUT_FOLDER, f)\n",
    "        row = process_image(full_path, meta_dict.get(f, {}))\n",
    "        if row: new_rows.append(row)\n",
    "\n",
    "    if new_rows:\n",
    "        df = pd.DataFrame(new_rows)\n",
    "        # Ensure correct column order matches DB\n",
    "        cols = [\"Timestamp\", \"Filename\", \"Camera_Serial\", \"Asset_Name\", \n",
    "                \"Max_Temp_C\", \"Min_Temp_C\", \"Center_Temp_C\", \"Avg_Temp_C\", \n",
    "                \"Delta_Temp_C\", \"Emissivity\", \"Distance\", \"Image_Base64\"]\n",
    "        df = df[cols]\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='mixed')\n",
    "        \n",
    "        try:\n",
    "            df.to_sql(DB_TABLE, engine, if_exists='append', index=False)\n",
    "            print(f\"üéâ SUCCESS: Uploaded {len(df)} new records.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Upload Failed: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: WATCHDOG EVENT HANDLER\n",
    "# ==============================================================================\n",
    "\n",
    "class NewImageHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event):\n",
    "        # Trigger when a new file is created\n",
    "        if not event.is_directory and event.src_path.lower().endswith(\".jpg\"):\n",
    "            print(f\"\\nüëÄ New file detected: {os.path.basename(event.src_path)}\")\n",
    "            time.sleep(1) # Wait for write to finish\n",
    "            run_pipeline()\n",
    "\n",
    "    def on_moved(self, event):\n",
    "        # Trigger when a file is renamed or moved into the folder\n",
    "        if not event.is_directory and event.dest_path.lower().endswith(\".jpg\"):\n",
    "            print(f\"\\nüëÄ File move detected: {os.path.basename(event.dest_path)}\")\n",
    "            time.sleep(1)\n",
    "            run_pipeline()\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(INPUT_FOLDER):\n",
    "        print(f\"‚ùå Folder '{INPUT_FOLDER}' does not exist. Please create it.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"‚úÖ WATCHING folder: '{INPUT_FOLDER}'\")\n",
    "    print(\"   - Drop files to trigger automatically.\")\n",
    "    print(\"   - Press [ENTER] to trigger manually.\")\n",
    "    print(\"   - Press Ctrl+C to stop.\")\n",
    "\n",
    "    # 1. Run once at startup\n",
    "    run_pipeline()\n",
    "\n",
    "    # 2. Start the Watcher in the background\n",
    "    event_handler = NewImageHandler()\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, INPUT_FOLDER, recursive=False)\n",
    "    observer.start()\n",
    "\n",
    "    try:\n",
    "        # CHANGED: Instead of sleeping, we wait for user input\n",
    "        while True:\n",
    "            input() # Waits for you to press Enter\n",
    "            print(\"\\nForce trigger requested...\")\n",
    "            run_pipeline()\n",
    "            print(\"\\n‚úÖ Done. Watching...\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "        print(\"\\nüõë Stopping Watcher.\")\n",
    "    \n",
    "    observer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd596022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to PSQLAPPEG297-01 to verify data...\n",
      "üì• Downloading full table data...\n",
      "‚úÖ Saved 'Database_Full_Dump.csv' (3 rows)\n",
      "‚úÖ Saved 'Local_Files_In_DB.csv' (3 rows found matching your local folder)\n",
      "üéâ SUCCESS: 100% of your local images are present in the database.\n"
     ]
    }
   ],
   "source": [
    "#################################### POST-UPLOAD VERIFICATION & EXPORT ####################\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# --- CONFIGURATION (Must match previous cell) ---\n",
    "INPUT_FOLDER = \"flir e5 photodump\"\n",
    "DB_SERVER = \"PSQLAPPEG297-01\"\n",
    "DB_NAME = \"Flir\"\n",
    "DB_USER = \"Flir\"\n",
    "DB_PASS = \"Prom@2025\"\n",
    "DB_TABLE = \"ThermalReadings\"\n",
    "\n",
    "def verify_and_export():\n",
    "    print(f\"üîå Connecting to {DB_SERVER} to verify data...\")\n",
    "    \n",
    "    # 1. Connect to MSSQL\n",
    "    try:\n",
    "        encoded_pass = quote_plus(DB_PASS)\n",
    "        db_url = f\"mssql+pyodbc://{DB_USER}:{encoded_pass}@{DB_SERVER}/{DB_NAME}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "        engine = create_engine(db_url)\n",
    "        \n",
    "        # 2. Download EVERYTHING from the Database\n",
    "        print(\"üì• Downloading full table data...\")\n",
    "        query = f\"SELECT * FROM {DB_TABLE} ORDER BY Timestamp DESC\"\n",
    "        df_full = pd.read_sql(query, engine)\n",
    "        \n",
    "        # Save File 1: The Full Database Dump\n",
    "        df_full.to_csv(\"Database_Full_Dump.csv\", index=False)\n",
    "        print(f\"‚úÖ Saved 'Database_Full_Dump.csv' ({len(df_full)} rows)\")\n",
    "\n",
    "        # 3. Verify Local Files\n",
    "        if os.path.exists(INPUT_FOLDER):\n",
    "            # Get list of filenames currently in your folder\n",
    "            local_files = [f for f in os.listdir(INPUT_FOLDER) if f.lower().endswith(\".jpg\")]\n",
    "            \n",
    "            # Filter the database data to see which of your local files made it in\n",
    "            # We check if the 'Filename' column in DB exists in our local file list\n",
    "            df_local_verified = df_full[df_full['Filename'].isin(local_files)]\n",
    "            \n",
    "            # Save File 2: The Verified Uploads\n",
    "            df_local_verified.to_csv(\"Local_Files_In_DB.csv\", index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Saved 'Local_Files_In_DB.csv' ({len(df_local_verified)} rows found matching your local folder)\")\n",
    "            \n",
    "            # Quick Integrity Check\n",
    "            missing_count = len(local_files) - len(df_local_verified)\n",
    "            if missing_count == 0:\n",
    "                print(\"üéâ SUCCESS: 100% of your local images are present in the database.\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è WARNING: {missing_count} local images are NOT in the database yet.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Could not find folder '{INPUT_FOLDER}' to verify local files.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Verification failed: {e}\")\n",
    "\n",
    "# Run verification\n",
    "verify_and_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93b058ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PSQLAPPEG297-01...\n",
      "‚úÖ Success! Table 'ThermalReadings' has been cleared.\n"
     ]
    }
   ],
   "source": [
    "##################################### DELETE ALL DATA FROM TABLE ####################\n",
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DB_SERVER = \"PSQLAPPEG297-01\"\n",
    "DB_NAME = \"Flir\"\n",
    "DB_USER = \"Flir\"\n",
    "DB_PASS = \"Prom@2025\"\n",
    "DB_TABLE = \"ThermalReadings\"\n",
    "\n",
    "def delete_table_data():\n",
    "    # 1. Safety Check\n",
    "    confirm = input(f\"‚ö†Ô∏è ARE YOU SURE you want to delete ALL rows from '{DB_TABLE}'? (Type 'yes' to confirm): \")\n",
    "    if confirm.lower() != \"yes\":\n",
    "        print(\"Action cancelled.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"Connecting to {DB_SERVER}...\")\n",
    "        \n",
    "        # 2. Encode Password & Connect\n",
    "        encoded_pass = quote_plus(DB_PASS)\n",
    "        db_url = f\"mssql+pyodbc://{DB_USER}:{encoded_pass}@{DB_SERVER}/{DB_NAME}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "        engine = create_engine(db_url)\n",
    "\n",
    "        # 3. Execute Delete\n",
    "        with engine.connect() as conn:\n",
    "            # Using 'DELETE' instead of 'TRUNCATE' is safer regarding permissions\n",
    "            # sql = text(f\"DELETE FROM {DB_TABLE} where Filename like 'FLIR0058.jpg'\")\n",
    "            sql = text(f\"DELETE FROM {DB_TABLE}\")\n",
    "            result = conn.execute(sql)\n",
    "            conn.commit()\n",
    "            print(f\"‚úÖ Success! Table '{DB_TABLE}' has been cleared.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deleting data: {e}\")\n",
    "\n",
    "# Run the delete function\n",
    "delete_table_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c122a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7ed064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning metadata (Batch)...\n",
      "New images to process: 0\n"
     ]
    }
   ],
   "source": [
    "#### QR Code ####\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import io\n",
    "import concurrent.futures \n",
    "import flyr\n",
    "from PIL import Image\n",
    "from pyzbar.pyzbar import decode\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "INPUT_FOLDER = os.path.join(base_dir, \"flir e5 photodump\")\n",
    "OUTPUT_CSV = os.path.join(base_dir, \"Thermal_Data_Log.csv\")\n",
    "EXIFTOOL_PATH = os.path.join(base_dir, \"exiftool-12.35.exe\") \n",
    "\n",
    "# Worker count: 10 is usually safe for file operations\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def get_process_flags():\n",
    "    if os.name == 'nt':\n",
    "        return subprocess.CREATE_NO_WINDOW\n",
    "    return 0\n",
    "\n",
    "def load_existing_records(csv_path):\n",
    "    existing_signatures = set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return existing_signatures, False \n",
    "    try:\n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                ts = str(row.get(\"Timestamp\", \"\")).strip()\n",
    "                sn = str(row.get(\"Serial Number\", \"\")).strip()\n",
    "                if ts and sn:\n",
    "                    existing_signatures.add((ts, sn))\n",
    "        return existing_signatures, True\n",
    "    except Exception:\n",
    "        return existing_signatures, False\n",
    "\n",
    "def get_folder_metadata_batch(folder_path, tool_path):\n",
    "    print(\"Scanning metadata (Batch)...\")\n",
    "    tool_cmd = tool_path if os.path.exists(tool_path) else \"exiftool\"\n",
    "    try:\n",
    "        cmd = [\n",
    "            tool_cmd, '-j', '-n', '-r', '-ext', 'jpg', '-ext', 'jpeg',\n",
    "            '-DateTimeOriginal', '-CameraModel', '-CameraSerialNumber',\n",
    "            '-SerialNumber', '-Emissivity', '-ObjectDistance',\n",
    "            '-ImageDescription', '-UserComment',\n",
    "            folder_path\n",
    "        ]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, creationflags=get_process_flags())\n",
    "        if result.stdout:\n",
    "            return json.loads(result.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in metadata extraction: {e}\")\n",
    "    return []\n",
    "\n",
    "# --- QR EXTRACTION HELPER ---\n",
    "def extract_qr_with_exiftool(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the 'EmbeddedImage' (Real Photo) from the FLIR file \n",
    "    using ExifTool, then scans it for QR codes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Command to extract the binary image data\n",
    "        cmd = [EXIFTOOL_PATH, \"-b\", \"-EmbeddedImage\", file_path]\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            cmd, \n",
    "            capture_output=True,\n",
    "            creationflags=get_process_flags()\n",
    "        )\n",
    "        \n",
    "        # If extraction worked, we have bytes\n",
    "        if result.stdout:\n",
    "            # Create a virtual image file in memory\n",
    "            with Image.open(io.BytesIO(result.stdout)) as img:\n",
    "                decoded = decode(img)\n",
    "                if decoded:\n",
    "                    # Return all found codes joined by pipe\n",
    "                    return \" | \".join([obj.data.decode(\"utf-8\") for obj in decoded])\n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "    return \"\" # Return empty string if nothing found or error\n",
    "\n",
    "# --- CORE PROCESSING WORKER ---\n",
    "def process_single_image(meta):\n",
    "    \"\"\"\n",
    "    Runs in a separate Thread. \n",
    "    \"\"\"\n",
    "    full_path = meta.get('SourceFile')\n",
    "    filename = os.path.basename(full_path)\n",
    "    \n",
    "    # 1. Basic Metadata\n",
    "    row_ts = str(meta.get(\"DateTimeOriginal\", \"Unknown\"))\n",
    "    row_model = str(meta.get(\"CameraModel\", \"Unknown\"))\n",
    "    row_sn = str(meta.get(\"CameraSerialNumber\", \"\"))\n",
    "    if not row_sn or row_sn == \"None\":\n",
    "        row_sn = str(meta.get(\"SerialNumber\", \"Unknown\"))\n",
    "        \n",
    "    # 2. Notes\n",
    "    note = meta.get(\"ImageDescription\")\n",
    "    if not note:\n",
    "        note = meta.get(\"UserComment\")\n",
    "    note_str = str(note).strip() if note else \"\"\n",
    "\n",
    "    qr_str = \"\"\n",
    "    status = \"Success\"\n",
    "    \n",
    "    # Thermal Data Placeholders\n",
    "    center_t, max_t, min_t, avg_t, delta_t = \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    try:\n",
    "        # --- A. QR PROCESSING ---\n",
    "        qr_str = extract_qr_with_exiftool(full_path)\n",
    "\n",
    "        # --- B. THERMAL PROCESSING ---\n",
    "        thermogram = flyr.unpack(full_path)\n",
    "        thermal_data = thermogram.celsius\n",
    "        \n",
    "        min_val = thermal_data.min()\n",
    "        max_val = thermal_data.max()\n",
    "        avg_val = thermal_data.mean()\n",
    "        delta_val = max_val - min_val\n",
    "        h, w = thermal_data.shape\n",
    "        center_val = thermal_data[h//2, w//2]\n",
    "\n",
    "        center_t = f\"{center_val:.1f}\"\n",
    "        max_t = f\"{max_val:.1f}\"\n",
    "        min_t = f\"{min_val:.1f}\"\n",
    "        avg_t = f\"{avg_val:.1f}\"\n",
    "        delta_t = f\"{delta_val:.1f}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        err_msg = str(e)\n",
    "        if \"not a FLIR\" in err_msg or \"Invalid\" in err_msg:\n",
    "            status = \"Error: Not Radiometric\"\n",
    "        else:\n",
    "            status = f\"Error: {err_msg}\"\n",
    "\n",
    "    return [\n",
    "        filename, row_ts, row_model, row_sn,\n",
    "        center_t, max_t, min_t, avg_t, delta_t,\n",
    "        meta.get(\"Emissivity\", \"N/A\"),\n",
    "        meta.get(\"ObjectDistance\", \"N/A\"),\n",
    "        note_str,\n",
    "        qr_str,\n",
    "        status\n",
    "    ]\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "def main():\n",
    "    if not os.path.exists(INPUT_FOLDER):\n",
    "        print(f\"Error: Input folder not found: {INPUT_FOLDER}\")\n",
    "        return\n",
    "\n",
    "    # 1. Load DB\n",
    "    existing_sigs, csv_exists = load_existing_records(OUTPUT_CSV)\n",
    "    \n",
    "    # 2. Batch Scan Metadata\n",
    "    all_metadata = get_folder_metadata_batch(INPUT_FOLDER, EXIFTOOL_PATH)\n",
    "    \n",
    "    # 3. Filter New Files\n",
    "    files_to_process = []\n",
    "    for meta in all_metadata:\n",
    "        if 'SourceFile' not in meta: continue\n",
    "        ts = str(meta.get(\"DateTimeOriginal\", \"\")).strip()\n",
    "        sn = str(meta.get(\"CameraSerialNumber\", \"\")).strip() or str(meta.get(\"SerialNumber\", \"\")).strip()\n",
    "        \n",
    "        if (ts, sn) not in existing_sigs:\n",
    "            files_to_process.append(meta)\n",
    "\n",
    "    total_new = len(files_to_process)\n",
    "    print(f\"New images to process: {total_new}\")\n",
    "    if total_new == 0: return\n",
    "\n",
    "    # 4. Threaded Processing\n",
    "    headers = [\n",
    "        \"File Name\", \"Timestamp\", \"Camera Model\", \"Serial Number\", \n",
    "        \"Center Temp (C)\", \"Max Temp (C)\", \"Min Temp (C)\", \n",
    "        \"Avg Temp (C)\", \"Delta Temp (C)\", \n",
    "        \"Emissivity\", \"Distance\", \"Notes\", \"QR Data\", \"Status\"\n",
    "    ]\n",
    "\n",
    "    with open(OUTPUT_CSV, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not csv_exists:\n",
    "            writer.writerow(headers)\n",
    "\n",
    "        print(f\"Starting parallel processing with {MAX_WORKERS} threads...\\n\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            future_to_meta = {executor.submit(process_single_image, meta): meta for meta in files_to_process}\n",
    "            \n",
    "            for i, future in enumerate(concurrent.futures.as_completed(future_to_meta), 1):\n",
    "                try:\n",
    "                    row = future.result()\n",
    "                    writer.writerow(row)\n",
    "                    f.flush() \n",
    "                    \n",
    "                    # --- UPDATED PROGRESS LINE ---\n",
    "                    # end='\\r' returns cursor to start of line\n",
    "                    # \" \"*20 adds blank space to clear any previous longer text\n",
    "                    filename = row[0]\n",
    "                    print(f\"Progress [{i}/{total_new}]: {filename}\" + \" \"*30, end='\\r', flush=True)\n",
    "                    \n",
    "                except Exception as exc:\n",
    "                    print(f\"\\nError processing file: {exc}\")\n",
    "\n",
    "    print(f\"\\nDone! Processed {total_new} files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
